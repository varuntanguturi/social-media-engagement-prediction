{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Engagement Prediction\n",
    "\n",
    "This notebook walks through the complete data science process for predicting social media engagement rates. We'll cover every step from data generation to model evaluation, with detailed explanations along the way.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll build regression models to predict social media engagement rates based on post characteristics such as length, number of images, hashtags, posting time, and follower count.\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to generate and preprocess synthetic social media data\n",
    "- Techniques for exploratory data analysis\n",
    "- How to build and evaluate different regression models\n",
    "- How to interpret model results and improve performance\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Environment\n",
    "\n",
    "First, let's import the necessary libraries and modules from our project. These include standard data science libraries like pandas and NumPy, as well as our custom modules for each step of the data science workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path to import from src\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_generation import generate_social_media_data\n",
    "from src.data_preprocessing import (\n",
    "    handle_missing_values, scale_features, engineer_time_features,\n",
    "    detect_and_handle_outliers, encode_categorical_variables, split_dataset\n",
    ")\n",
    "from src.exploratory_analysis import (\n",
    "    generate_descriptive_statistics, plot_feature_distributions, plot_correlation_heatmap,\n",
    "    plot_target_correlations, plot_scatter_with_target, plot_time_based_analysis, plot_feature_pairplot\n",
    ")\n",
    "from src.model_building import (\n",
    "    train_linear_regression, train_random_forest, train_gradient_boosting,\n",
    "    tune_hyperparameters, perform_cross_validation, get_feature_importances\n",
    ")\n",
    "from src.model_evaluation import (\n",
    "    calculate_regression_metrics, print_metrics_report, plot_actual_vs_predicted,\n",
    "    plot_residuals, plot_feature_importance, plot_learning_curve, compare_models_plot\n",
    ")\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "# Display versions\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"numpy version: {np.__version__}\")\n",
    "print(f\"matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "In a real-world scenario, you would typically collect social media data through APIs or datasets. For this tutorial, we'll generate synthetic data that follows realistic patterns of social media engagement.\n",
    "\n",
    "### Understanding the Data Features\n",
    "\n",
    "Our synthetic data will include the following features:\n",
    "\n",
    "- **post_length**: Number of characters in the post (range: 10-500)\n",
    "- **images_count**: Number of images in the post (range: 0-10)\n",
    "- **hashtags_count**: Number of hashtags in the post (range: 0-30)\n",
    "- **post_hour**: Hour of the day when the post was published (0-24 as a float)\n",
    "- **post_day**: Day of week when the post was published (0=Monday, 6=Sunday)\n",
    "- **followers_count**: Number of followers the account has (log-normal distribution)\n",
    "- **engagement_rate**: Target variable - percentage of followers who engaged with the post\n",
    "\n",
    "The engagement rate is calculated based on all these features with some added randomness to simulate real-world variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic social media data\n",
    "# We'll create 1,000 samples with some missing values and noise\n",
    "\n",
    "df = generate_social_media_data(\n",
    "    n_samples=1000,\n",
    "    output_file=\"../data/social_media_data.csv\",\n",
    "    random_seed=42,\n",
    "    missing_data_pct=0.05,  # 5% missing values\n",
    "    noise_level=0.1         # 10% noise in the target variable\n",
    ")\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Generated {len(df)} samples with {df.shape[1]} features\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data Generation Process\n",
    "\n",
    "Let's examine how the data was generated. In our `generate_social_media_data` function:\n",
    "\n",
    "1. **post_length**: Generated from a uniform distribution (10-500 characters)\n",
    "2. **images_count**: Follows a probability distribution where most posts have 0-2 images\n",
    "3. **hashtags_count**: Uses a distribution where moderate hashtag counts are more common\n",
    "4. **followers_count**: Uses a log-normal distribution to simulate realistic follower counts (many accounts with few followers, few accounts with many followers)\n",
    "5. **post_hour**: Distributed throughout the day as a continuous value\n",
    "6. **post_day**: Uniform distribution across the week (0-6)\n",
    "\n",
    "The **engagement_rate** is calculated using a formula that considers:\n",
    "- Diminishing returns for very long posts\n",
    "- Positive impact of images up to a point\n",
    "- Initial positive then negative impact of hashtags (too many can reduce engagement)\n",
    "- Negative correlation with follower count (larger accounts often have lower engagement rates)\n",
    "- Time-of-day effects (higher engagement in evenings and mornings)\n",
    "- Day-of-week effects (higher on weekends)\n",
    "\n",
    "Let's check the basic statistics of our generated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_counts,\n",
    "    'Percent Missing': missing_percent\n",
    "})\n",
    "\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Exploring the Data Generation Parameters\n",
    "\n",
    "Try generating datasets with different parameters to see how they affect the data distribution and engagement rates.\n",
    "\n",
    "1. Generate a dataset with higher noise level (e.g., 0.3)\n",
    "2. Generate a dataset with more missing values (e.g., 0.15)\n",
    "3. Compare the statistical properties of these datasets with the original one\n",
    "\n",
    "Note: Uncomment and modify the code below to complete the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Generate data with higher noise\n",
    "# df_high_noise = generate_social_media_data(\n",
    "#     n_samples=1000,\n",
    "#     random_seed=42,\n",
    "#     noise_level=0.3,  # Higher noise\n",
    "#     missing_data_pct=0.05\n",
    "# )\n",
    "# \n",
    "# # Compare the target variable distribution\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# \n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.hist(df['engagement_rate'], bins=30, alpha=0.7)\n",
    "# plt.title('Original Data (Noise=0.1)')\n",
    "# plt.xlabel('Engagement Rate')\n",
    "# \n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.hist(df_high_noise['engagement_rate'], bins=30, alpha=0.7)\n",
    "# plt.title('High Noise Data (Noise=0.3)')\n",
    "# plt.xlabel('Engagement Rate')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Now that we have our data, we need to prepare it for modeling. Data preprocessing is a critical step that can significantly impact model performance. We'll cover several key preprocessing tasks:\n",
    "\n",
    "1. Handling missing values\n",
    "2. Feature engineering\n",
    "3. Detecting and handling outliers\n",
    "4. Feature scaling\n",
    "5. Data splitting\n",
    "\n",
    "Let's start with handling missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Handling Missing Values\n",
    "\n",
    "Missing values are common in real-world datasets and need to be handled appropriately. There are several strategies:\n",
    "\n",
    "- **Mean/Median/Mode Imputation**: Replace missing values with the mean, median, or mode of the column\n",
    "- **KNN Imputation**: Replace missing values using values from K nearest neighbors\n",
    "- **Forward/Backward Fill**: Use adjacent values in time series data\n",
    "- **Dropping**: Remove rows or columns with missing values (generally not recommended unless missing data is very limited)\n",
    "\n",
    "For our dataset, we'll use mean imputation for simplicity, but will discuss alternatives as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using mean imputation\n",
    "df_imputed = handle_missing_values(df, strategy='mean')\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(f\"Missing values after imputation: {df_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative Approaches for Missing Value Imputation**\n",
    "\n",
    "Mean imputation is simple but has limitations. It doesn't consider relationships between features and can distort the distribution. Here are some alternatives we could use:\n",
    "\n",
    "1. **Median Imputation**: More robust to outliers\n",
    "2. **KNN Imputation**: Considers relationships between data points\n",
    "\n",
    "Let's compare these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different imputation strategies\n",
    "df_median = handle_missing_values(df, strategy='median')\n",
    "df_knn = handle_missing_values(df, strategy='knn', knn_neighbors=5)\n",
    "\n",
    "# Compare distributions for post_length after different imputation methods\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df_imputed['post_length'].dropna(), kde=True)\n",
    "plt.title('Mean Imputation')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(df_median['post_length'].dropna(), kde=True)\n",
    "plt.title('Median Imputation')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(df_knn['post_length'].dropna(), kde=True)\n",
    "plt.title('KNN Imputation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering\n",
    "\n",
    "Feature engineering involves creating new features from existing ones to improve model performance. For time-based data like social media posts, we can extract additional insights from the posting time.\n",
    "\n",
    "Let's create additional time-related features from our `post_hour` and `post_day` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer time-based features\n",
    "df_engineered = engineer_time_features(df_imputed)\n",
    "\n",
    "# Display the new features\n",
    "new_columns = [col for col in df_engineered.columns if col not in df_imputed.columns]\n",
    "print(f\"Newly created features: {new_columns}\")\n",
    "df_engineered[new_columns + ['post_hour', 'post_day']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Engineered Features**\n",
    "\n",
    "We've created several new time-based features:\n",
    "\n",
    "1. **hour_sin** and **hour_cos**: These capture the cyclical nature of time (hour 23 is close to hour 0). Using sine and cosine transformations preserves this cyclical relationship.\n",
    "\n",
    "2. **time_of_day**: A categorical feature that divides the day into morning, afternoon, evening, and night.\n",
    "\n",
    "3. **is_peak_hour**: A binary feature indicating typically high-engagement hours (morning commute and evening hours).\n",
    "\n",
    "4. **day_sin** and **day_cos**: Similar to the hour features, these capture the cyclical nature of days of the week.\n",
    "\n",
    "5. **is_weekend**: A binary feature indicating whether the post was made on a weekend.\n",
    "\n",
    "Let's visualize how engagement varies across these new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engagement rate by time_of_day\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x='time_of_day', y='engagement_rate', data=df_engineered)\n",
    "plt.title('Engagement Rate by Time of Day')\n",
    "plt.show()\n",
    "\n",
    "# Visualize engagement rate by weekend vs weekday\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='is_weekend', y='engagement_rate', data=df_engineered)\n",
    "plt.title('Engagement Rate: Weekday vs Weekend')\n",
    "plt.xticks([0, 1], ['Weekday', 'Weekend'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Detecting and Handling Outliers\n",
    "\n",
    "Outliers are extreme values that deviate significantly from other observations and can distort model training. Let's detect and handle outliers in our features.\n",
    "\n",
    "Common methods for outlier detection:\n",
    "1. **IQR (Interquartile Range) Method**: Identify values below Q1 - 1.5×IQR or above Q3 + 1.5×IQR\n",
    "2. **Z-score Method**: Identify values more than n standard deviations from the mean\n",
    "3. **Isolation Forest**: Machine learning algorithm that isolates observations by randomly selecting a feature and a split value\n",
    "\n",
    "Let's use the IQR method to detect outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using the IQR method and handle them by clipping\n",
    "df_no_outliers, outlier_summary = detect_and_handle_outliers(\n",
    "    df_engineered,\n",
    "    method='iqr',\n",
    "    treatment='clip'\n",
    ")\n",
    "\n",
    "# Display outlier summary\n",
    "print(\"Outlier Detection Summary:\")\n",
    "outlier_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the effect of outlier treatment on one of the features with many outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a feature with outliers (e.g., followers_count)\n",
    "feature = 'followers_count'\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(y=df_engineered[feature])\n",
    "plt.title(f'{feature} Before Outlier Treatment')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(y=df_no_outliers[feature])\n",
    "plt.title(f'{feature} After Outlier Treatment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare distributions\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_engineered[feature], kde=True, bins=30)\n",
    "plt.title(f'{feature} Before Outlier Treatment')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_no_outliers[feature], kde=True, bins=30)\n",
    "plt.title(f'{feature} After Outlier Treatment')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative Outlier Treatment Approaches**\n",
    "\n",
    "We used value clipping, which caps extreme values at the outlier thresholds. Other approaches include:\n",
    "\n",
    "1. **Removing outliers**: Eliminates outlier data points entirely\n",
    "2. **Transformation**: Apply log, square root, or other transformations to compress the range\n",
    "3. **Binning**: Group values into bins to reduce the impact of extreme values\n",
    "\n",
    "The best approach depends on your specific dataset and problem context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Scaling\n",
    "\n",
    "Feature scaling is important for many machine learning algorithms, especially those that rely on distances or gradients. By standardizing features to have similar scales, we can improve model training stability and convergence.\n",
    "\n",
    "Common scaling techniques include:\n",
    "1. **Standardization** (Z-score normalization): Transforms features to have mean=0 and std=1\n",
    "2. **Min-Max Scaling**: Scales features to a specific range, typically [0,1]\n",
    "3. **Robust Scaling**: Uses statistics that are robust to outliers (median and IQR)\n",
    "\n",
    "Let's apply standardization to our numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization to all numeric features except the target variable\n",
    "cols_to_scale = [col for col in df_no_outliers.columns if col != 'engagement_rate' \n",
    "                and df_no_outliers[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "df_scaled, scaler = scale_features(\n",
    "    df_no_outliers,\n",
    "    method='standard',\n",
    "    columns=cols_to_scale\n",
    ")\n",
    "\n",
    "# Compare original and scaled data for a few features\n",
    "compare_cols = ['post_length', 'followers_count', 'hashtags_count']\n",
    "comparison = pd.DataFrame()\n",
    "\n",
    "for col in compare_cols:\n",
    "    comparison[f'{col}_original'] = df_no_outliers[col]\n",
    "    comparison[f'{col}_scaled'] = df_scaled[col]\n",
    "    \n",
    "comparison.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how after standardization, each feature has a mean close to 0 and a standard deviation close to 1. This will help algorithms that are sensitive to feature scales, like linear regression or k-nearest neighbors.\n",
    "\n",
    "Let's visualize the effect of scaling on a couple of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original distributions\n",
    "plt.subplot(1, 2, 1)\n",
    "for col in compare_cols:\n",
    "    sns.kdeplot(df_no_outliers[col], label=col)\n",
    "plt.title('Original Distributions')\n",
    "plt.legend()\n",
    "\n",
    "# Scaled distributions\n",
    "plt.subplot(1, 2, 2)\n",
    "for col in compare_cols:\n",
    "    sns.kdeplot(df_scaled[col], label=col)\n",
    "plt.title('Scaled Distributions')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Encoding Categorical Variables\n",
    "\n",
    "Many machine learning algorithms require numerical input. If we have categorical variables, we need to encode them as numbers. Our feature engineering step created the categorical feature `time_of_day`, which needs encoding.\n",
    "\n",
    "Common encoding techniques include:\n",
    "1. **One-hot encoding**: Creates binary columns for each category\n",
    "2. **Label encoding**: Assigns a unique integer to each category\n",
    "3. **Target encoding**: Replaces categories with the mean target value for that category\n",
    "\n",
    "Let's apply one-hot encoding to our categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns\n",
    "cat_columns = [col for col in df_scaled.columns if df_scaled[col].dtype == 'object']\n",
    "print(f\"Categorical columns: {cat_columns}\")\n",
    "\n",
    "# Apply one-hot encoding\n",
    "df_encoded = encode_categorical_variables(df_scaled, columns=cat_columns, method='one_hot')\n",
    "\n",
    "# Compare data shapes before and after encoding\n",
    "print(f\"Shape before encoding: {df_scaled.shape}\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Display new one-hot encoded columns\n",
    "new_cols = [col for col in df_encoded.columns if col not in df_scaled.columns]\n",
    "print(f\"\\nNewly created one-hot encoded columns: {new_cols}\")\n",
    "df_encoded[new_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Data Splitting\n",
    "\n",
    "The final preprocessing step is to split our data into training and testing sets. This allows us to train our models on one set of data and evaluate them on unseen data, giving a better estimate of how they'll perform on new data.\n",
    "\n",
    "We'll use a standard 70-15-15 split:\n",
    "- 70% for training\n",
    "- 15% for validation (hyperparameter tuning)\n",
    "- 15% for testing (final evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(\n",
    "    df_encoded,\n",
    "    target_column='engagement_rate',\n",
    "    test_size=0.15,\n",
    "    val_size=0.15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Verify that the target distributions are similar across splits\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(y_train, kde=True, bins=20)\n",
    "plt.title('Training Set')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(y_val, kde=True, bins=20)\n",
    "plt.title('Validation Set')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(y_test, kde=True, bins=20)\n",
    "plt.title('Test Set')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Preprocessing Experimentation\n",
    "\n",
    "Try changing some of the preprocessing steps to see how they affect the data. Here are some suggestions:\n",
    "\n",
    "1. Use a different imputation strategy (e.g., KNN instead of mean)\n",
    "2. Try a different outlier detection method (e.g., isolation_forest)\n",
    "3. Use a different scaling method (e.g., minmax instead of standard)\n",
    "4. Experiment with different train/test split ratios\n",
    "\n",
    "Note: Uncomment and modify the code below to complete the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Try different preprocessing approaches\n",
    "# \n",
    "# # 1. Use KNN imputation\n",
    "# df_knn_imputed = handle_missing_values(df, strategy='knn', knn_
